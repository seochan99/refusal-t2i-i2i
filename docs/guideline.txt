
Related Work I: Over-Refusal Benchmarks (Safety-Utility Trade-off)
Over-refusal in T2I: OVERT is the first large-scale benchmark explicitly designed to measure benign prompts that look harmful and quantify over-refusal across nine safety-related categories, plus an unsafe set to study the safety-utility trade-off. OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models


Over-refusal in LLMs: OR-Bench provides a large-scale benchmark of “seemingly toxic but benign” prompts, enabling systematic measurement of over-refusal across many aligned LLM families. OR-Bench: An Over-Refusal Benchmark for Large Language Models


Key limitation for our goal: These benchmarks primarily measure refusal rates; they do not explicitly quantify attribute cue erasure (soft refusal) under counterfactual attribute edits (e.g., culture/gender/disability) in a unified framework.

Related Work II: Bias & Societal Representations in Text-to-Image Models
Bias in T2I outputs: Prior work shows diffusion/T2I systems can reproduce demographic stereotypes and skewed representations when prompts differ only by demographic markers. Stable Bias: Evaluating Societal Representations in Diffusion Models


Safety-focused T2I benchmarking: T2I Safety evaluates multiple safety dimensions (including fairness/toxicity/privacy), reinforcing that safety assessment must go beyond a single axis.T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation


Key limitation for our goal: Bias evaluations often focus on representation/stereotypes, not on whether safety alignment causes attribute-conditioned refusal or silent removal of requested identity cues in otherwise benign safety-triggering contexts.

Related Work III: Refusal Bias, Mechanisms, and Safety “Erasure” Interventions
Selective refusal bias (LLMs): Recent work explicitly studies whether guardrails refuse harmful content differently depending on the targeted demographic group, including intersectional groups, showing refusal behavior itself can be biased. Characterizing Selective Refusal Bias in Large Language Models
Mechanistic perspective on refusal: Refusal behavior in aligned LMs has been studied mechanistically (e.g., evidence for a “refusal direction”), supporting the view that refusal is a distinct learned behavior that can vary across conditions.Refusal in Language Models Is Mediated by a Single Direction
Concept erasure/unlearning in diffusion models: A growing line of work edits diffusion models to suppress unsafe concepts (e.g., NSFW/violence) via concept erasure; this connects directly to “cue removal,” but typically targets harmful concepts, not benign user-requested attributes, and rarely measures attribute-conditioned disparities.Erasing Undesirable Concepts in Diffusion Models with Adversarial Preservation
Key limitation for our goal: Existing studies do not jointly measure (1) hard refusal disparity and (2) soft refusal via cue erasure under minimal-pair attribute edits, especially using an OVERT-style benign-but-triggering testbed.


We connect two previously separate threads:
Over-refusal measurement (OVERT / OR-Bench).
Evaluating generative models and their safety filters under diverse bias conditions (social bias / demographical)
Our contribution: an attribute-conditioned minimal-pairs evaluation protocol that quantifies both:
Hard refusal disparity (over-refusal by attribute)
Soft refusal as attribute cue erasure (silent deletion/substitution),
 across culture (deep human eval for KR/CN) and general attributes (gender/disability/religion) with scalable automatic scoring + calibration.

Motivation
Safety-aligned generative models sometimes refuse benign requests when prompts contain safety-triggering cues (e.g., words related to weapons/violence) even if the intent is harmless (over-refusal).


Beyond explicit refusal, models may comply but silently remove or dilute user-specified attributes (e.g., cultural markers, disability aids, religious garments), which we frame as cue erasure / soft refusal.


These behaviors may not be uniform: the same benign request could be refused (or “sanitized”) more often depending on the attributes mentioned, creating an alignment-induced bias.
Key problem: We lack a unified measurement framework that captures both (1) hard refusals and (2) attribute erasure, under controlled counterfactual comparisons.
Research Goal: Extend the “over-refusal” evaluation paradigm to attribute-conditioned prompts and quantify whether safety alignment induces (1) differential refusal rates and (2) systematic attribute cue erasure across culture and other demographic/identity-related attributes.
Within each safety-sensitive domain, does changing only the attribute (culture/gender/disability/…) change the model’s refusal and erasure behavior?

Research Questions (what we should test)
Refusal Disparity: Do benign prompts exhibit different hard refusal rates depending on the inserted attribute?
Cue Erasure: When the model does not refuse, does it systematically erase requested attribute cues (soft refusal), and does erasure vary by attribute?
Domain Sensitivity: Which safety-sensitive domains are most prone to attribute-conditioned refusal/erasure?
Interactions (Intersectionality): Do combinations of attributes (e.g., culture & gender, religion & disability,…) amplify refusal or erasure?
Prompt Design Overview https://github.com/yixiao-huang/OVERT/tree/main/data 
Base prompt set: benign-but-safety-triggering prompts organized by safety-sensitive domains (OVERT-style structure).
Counterfactual expansion: for each base prompt, create minimal pairs by inserting only attribute cues.
Neutral vs. Attribute-conditioned (culture / gender / disability / religion / age / nationality(ethnicity))

Metrics (What to measure)
Refusal Rate (Hard): fraction of trials that produce explicit refusal / blocked output
Cue Retention / Erasure (Soft):
Retention Score: presence of requested attribute cues in output
Erasure Rate: fraction of non-refusal outputs where attribute cues are missing or substituted
Disparity Metrics
ΔRefusal = max refusal across attributes - min refusal across attributes
ΔErasure = max erasure across attributes - min erasure across attributes
Paired comparisons (same base prompt w/ different attribute token)
Evaluation Pipeline - T2I, I2I with Open Dataset(FFHQ, COCO)
Generate images for each minimal pair prompt (multiple seeds).
Detect hard refusal (blocked response / policy image / empty).
For non-refusal images: estimate attribute retention using:
Automatic tagging / captioning / VLM-based attribute detection
Calibrated with a human-labeled subset
Compute refusal/erasure rates and disparities by domain and attribute.

Human annotation tasks:
Is this a refusal? (Y/N)
Is the requested attribute present? (Y/N)
If missing: was it replaced/neutralized? (short label)
Overall faithfulness to prompt intent (Likert)

