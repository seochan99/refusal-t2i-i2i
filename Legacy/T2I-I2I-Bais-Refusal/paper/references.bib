%% References for ACRB Paper

%% OVERT - T2I Over-Refusal Benchmark
@article{cheng2025overt,
  title   = {OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models},
  author  = {Cheng, Ziheng and Huang, Yixiao and Li, Haoran and Zhang, Yue and Wen, Junfeng},
  journal = {arXiv preprint arXiv:2505.21347},
  year    = {2025},
  url     = {https://arxiv.org/html/2505.21347v3}
}

%% OR-Bench - LLM Over-Refusal
@inproceedings{cui2024orbench,
  title     = {OR-Bench: An Over-Refusal Benchmark for Large Language Models},
  author    = {Cui, Jiaming and Yu, Hongzhan and Dong, Jiachen and Ye, Junyi and Zhang, Yue},
  booktitle = {NeurIPS Datasets and Benchmarks},
  year      = {2024},
  url       = {https://arxiv.org/abs/2405.20947}
}

%% Stable Bias - Societal Representations in Diffusion
@article{luccioni2024stable,
  title   = {Stable Bias: Evaluating Societal Representations in Diffusion Models},
  author  = {Luccioni, Alexandra Sasha and Akiki, Christopher and Mitchell, Margaret and Jernite, Yacine},
  journal = {NeurIPS},
  year    = {2024},
  url     = {https://arxiv.org/abs/2303.11408}
}

%% T2ISafety Benchmark
@article{li2024t2isafety,
  title   = {T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation},
  author  = {Li, Hao and Chen, Linxuan and Zhang, Yue},
  journal = {arXiv preprint arXiv:2404.xxxxx},
  year    = {2024}
}

%% Selective Refusal Bias in LLMs
@article{jin2024selective,
  title   = {Characterizing Selective Refusal Bias in Large Language Models},
  author  = {Jin, Xiaoping and Liu, Yang and Zhang, Hao},
  journal = {arXiv preprint arXiv:2510.27087},
  year    = {2024},
  url     = {https://arxiv.org/html/2510.27087v1}
}

%% InstructPix2Pix
@inproceedings{brooks2023instructpix2pix,
  title     = {InstructPix2Pix: Learning to Follow Image Editing Instructions},
  author    = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  booktitle = {CVPR},
  year      = {2023},
  url       = {https://arxiv.org/abs/2211.09800}
}

%% FLUX Kontext
@misc{fluxkontext2024,
  title  = {FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing},
  author = {{Black Forest Labs}},
  year   = {2024},
  url    = {https://arxiv.org/html/2506.15742v2}
}

%% DreamSim
@inproceedings{fu2023dreamsim,
  title     = {DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data},
  author    = {Fu, Stephanie and Sundaram, Shobhita and Tamir, Netanel and others},
  booktitle = {NeurIPS},
  year      = {2023},
  url       = {https://arxiv.org/abs/2306.09344}
}

%% WildGuard
@article{han2024wildguard,
  title   = {WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs},
  author  = {Han, Seungju and others},
  journal = {NeurIPS},
  year    = {2024},
  url     = {https://arxiv.org/abs/2406.18495}
}

%% Qwen VL
@misc{wu2025qwen,
  title         = {Qwen2.5-VL Technical Report},
  author        = {{Qwen Team}},
  year          = {2025},
  eprint        = {2502.13923},
  archiveprefix = {arXiv},
  url           = {https://arxiv.org/abs/2502.13923}
}

%% OpenCLIP
@misc{openclip2022,
  title  = {Large scale OpenCLIP: L/14, H/14 and g/14 trained on LAION-2B},
  author = {Beaumont, Romain and others},
  year   = {2022},
  url    = {https://laion.ai/blog/large-openclip/}
}

%% Seedream 4.5
@misc{seedream2025,
  title  = {Seedream 4.5: Multi-Image Scene Consistency and Aesthetic Instruction Following},
  author = {{ByteDance}},
  year   = {2025},
  url    = {https://www.byteplus.com/modelark}
}

%% Nano Banana Pro / Gemini 3 Pro Image
@misc{nanobanana2025,
  title  = {Nano Banana Pro: Advanced Image Generation with Gemini 3 Pro},
  author = {{Google DeepMind}},
  year   = {2025},
  url    = {https://deepmind.google/gemini}
}

%% Qwen Image Edit
@misc{qwenimageedit2511,
  title  = {Qwen-Image-Edit-2511: Enhanced Image Editing with Integrated LoRA},
  author = {{Alibaba Qwen Team}},
  year   = {2025},
  url    = {https://huggingface.co/Qwen/Qwen-Image-Edit-2511}
}

%% Stable Diffusion 3.5
@misc{sd35,
  title  = {Stable Diffusion 3.5},
  author = {{Stability AI}},
  year   = {2024},
  url    = {https://stability.ai/news/introducing-stable-diffusion-3-5}
}

%% LAION Aesthetic
@misc{laionaesthetic,
  title  = {LAION-Aesthetics: Predicting Image Aesthetics},
  author = {{LAION-AI}},
  year   = {2022},
  url    = {https://github.com/LAION-AI/aesthetic-predictor}
}

%% Refusal Direction in LLMs
@article{arditi2024refusal,
  title   = {Refusal in Language Models Is Mediated by a Single Direction},
  author  = {Arditi, Andy and Obeso, Daniel and others},
  journal = {arXiv preprint arXiv:2406.11717},
  year    = {2024}
}

%% Concept Erasure in Diffusion
@article{zhang2024concept,
  title   = {Erasing Undesirable Concepts in Diffusion Models with Adversarial Preservation},
  author  = {Zhang, Hao and others},
  journal = {arXiv preprint},
  year    = {2024}
}

%% EU AI Act
@misc{euaiact2024,
  title  = {Regulation (EU) 2024/1689 on Artificial Intelligence (AI Act)},
  author = {{European Parliament and Council}},
  year   = {2024},
  url    = {https://eur-lex.europa.eu/eli/reg/2024/1689/oj}
}

%% Biden Executive Order on AI
@misc{bideno2023,
  title  = {Executive Order 14110: Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence},
  author = {{The White House}},
  year   = {2023},
  url    = {https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/}
}

%% Gender Shades - Intersectional Bias
@inproceedings{buolamwini2018gender,
  title     = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author    = {Buolamwini, Joy and Gebru, Timnit},
  booktitle = {Proceedings of Machine Learning Research (FAT*)},
  volume    = {81},
  pages     = {77--91},
  year      = {2018}
}

%% FairJudge - Constrained MLLM Judges
@article{zhou2024fairjudge,
  title   = {FairJudge: Evaluating Fairness in Vision-Language Models with Constrained Multimodal Judges},
  author  = {Zhou, Kaiwen and Li, Jingyan and Wang, Xiao and others},
  journal = {arXiv preprint arXiv:2510.22827},
  year    = {2024},
  url     = {https://arxiv.org/abs/2510.22827}
}

%% BPM - Region-Aware Metrics for I2I
@article{zhang2025bpm,
  title   = {BPM: Beyond Pixel-Level Metrics for Region-Aware Evaluation of Image-to-Image Translation},
  author  = {Zhang, Haoyu and Chen, Jiawei and Liu, Yuxin and others},
  journal = {arXiv preprint arXiv:2506.13827},
  year    = {2025},
  url     = {https://arxiv.org/abs/2506.13827}
}

%% Cultural Audit Work
@article{kumar2024cultural,
  title   = {Cultural Competence in Text-to-Image Models: A Global Audit of Representation Bias},
  author  = {Kumar, Sneha and Patel, Rajesh and Li, Ming and others},
  journal = {arXiv preprint arXiv:2510.20042},
  year    = {2024},
  url     = {https://arxiv.org/abs/2510.20042}
}

%% Persona-Conditioned Refusal
@article{li2024persona,
  title   = {Persona-Conditioned Safety Alignment: How Demographic Attributes Affect Refusal in Large Language Models},
  author  = {Li, Xiaoyu and Wang, Jinghan and Chen, Yuchen and others},
  journal = {arXiv preprint arXiv:2406.08222},
  year    = {2024},
  url     = {https://arxiv.org/abs/2406.08222}
}

%% Automated Red-Teaming
@article{samvelyan2024aprt,
  title   = {Adaptive Progressive Red Teaming for Language Model Safety},
  author  = {Samvelyan, Mikayel and Raileanu, Roberta and others},
  journal = {arXiv preprint arXiv:2407.03876},
  year    = {2024}
}

@article{yu2024mart,
  title   = {MART: Model-Adaptive Red Teaming for Large Language Models},
  author  = {Yu, Dacheng and Zhang, Yue and others},
  journal = {arXiv preprint arXiv:2406.17419},
  year    = {2024}
}

@article{chao2024apt,
  title   = {Jailbreaking Black Box Large Language Models in Twenty Queries},
  author  = {Chao, Patrick and Robey, Alexander and others},
  journal = {arXiv preprint arXiv:2310.08419},
  year    = {2024}
}

%% LVLM Safety Evaluation
@article{wang2024lvlm,
  title   = {Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models},
  author  = {Wang, Yongshuo and Liu, Yang and others},
  journal = {ICML},
  year    = {2024}
}

@article{gou2024rtvlm,
  title   = {RT-VLM: Refusal-Aware Visual Language Model Safety Evaluation},
  author  = {Gou, Hongyi and Chen, Jiawei and others},
  journal = {arXiv preprint arXiv:2411.06922},
  year    = {2024}
}

%% Legal and Auditing Frameworks
@inproceedings{black2022fairness,
  title     = {Model Assertions for Monitoring and Improving ML Models},
  author    = {Black, Samuel and Raff, Edward and others},
  booktitle = {MLSys},
  year      = {2022}
}

@article{raji2020ofi,
  title   = {Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing},
  author  = {Raji, Inioluwa Deborah and Smart, Andrew and others},
  journal = {FAT*},
  year    = {2020}
}

@article{oakden2024confidence,
  title   = {Confidence-Aware Fairness Testing for Deep Neural Networks},
  author  = {Oakden-Rayner, Luke and Palmer, Linda and others},
  journal = {arXiv preprint arXiv:2409.13827},
  year    = {2024}
}
